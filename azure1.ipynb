{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9238c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from docx import Document as DocxDocument\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.docstore.document import Document\n",
    "import faiss\n",
    "import numpy as np\n",
    "import tempfile, os, re, pickle\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "429b9ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8d7814c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (1.44.1)\n",
      "Requirement already satisfied: pyPDF2 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (5.5.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (2.1.3)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (19.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (2.32.5)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (4.0.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.31.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: openai in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (2.13.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langchain) (1.2.2)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langchain) (1.0.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.5.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.0)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from faiss-cpu) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: azure-storage-blob in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (12.27.1)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from azure-storage-blob) (1.37.0)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from azure-storage-blob) (44.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from azure-storage-blob) (4.15.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from azure-storage-blob) (0.7.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from azure-core>=1.30.0->azure-storage-blob) (2.32.5)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2025.11.12)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kalyan\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# Core packages\n",
    "!pip install streamlit pyPDF2\n",
    "\n",
    "# LangChain packages\n",
    "!pip install langchain openai faiss-cpu\n",
    "\n",
    "# Azure Blob Storage\n",
    "!pip install azure-storage-blob\n",
    "\n",
    "# Optional for HuggingFace embeddings (if you want to remove OpenAI dependency later)\n",
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c766248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Azure Blob Config\n",
    "# ==========================\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = Path.cwd() / \".env\"\n",
    "load_dotenv(env_path)\n",
    "\n",
    "AZURE_CONNECTION_STRING = os.getenv(\"AZURE_CONNECTION_STRING\")\n",
    "CONTAINER_NAME = os.getenv(\"CONTAINER_NAME\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b91d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================\n",
    "# Text cleaning\n",
    "# ==========================\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e621ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Load documents (PDF + DOCX + TXT)\n",
    "# ==========================\n",
    "documents_text = []\n",
    "\n",
    "for blob in container_client.list_blobs():\n",
    "    if not blob.name.lower().endswith((\".pdf\", \".docx\", \".txt\")):\n",
    "        continue\n",
    "\n",
    "    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(blob.name)[1])\n",
    "    tmp_file.write(container_client.get_blob_client(blob.name).download_blob().readall())\n",
    "    tmp_file.close()\n",
    "\n",
    "    ext = os.path.splitext(blob.name)[1].lower()\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        text = \"\\n\".join([p.page_content for p in PyPDFLoader(tmp_file.name).load()])\n",
    "\n",
    "    elif ext == \".docx\":\n",
    "        doc = DocxDocument(tmp_file.name)\n",
    "        text = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "\n",
    "    elif ext == \".txt\":\n",
    "        with open(tmp_file.name, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "    os.remove(tmp_file.name)\n",
    "    documents_text.append(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2cbec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Chunking\n",
    "# ==========================\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_chunks = []\n",
    "for txt in documents_text:\n",
    "    all_chunks.extend(splitter.split_text(txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b462e6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 67\n"
     ]
    }
   ],
   "source": [
    "print(\"Total chunks:\", len(all_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c06576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Embeddings + FAISS\n",
    "# ==========================\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    api_key=GEMINI_API_KEY\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3472983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(page_content=c) for c in all_chunks]\n",
    "embeddings = embedding_model.embed_documents([d.page_content for d in documents])\n",
    "embeddings_np = np.array(embeddings, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b37196b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faiss_index = faiss.IndexFlatL2(embeddings_np.shape[1])\n",
    "faiss_index.add(embeddings_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bb44da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(faiss_index, \"faiss_index.idx\")\n",
    "with open(\"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa4b1fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Helpers\n",
    "# ==========================\n",
    "def load_faiss():\n",
    "    return faiss.read_index(\"faiss_index.idx\"), pickle.load(open(\"chunks.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "802380f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_query(q):\n",
    "    return np.array(\n",
    "        embedding_model.embed_query(q),\n",
    "        dtype=\"float32\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c3c2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(q_emb, index, chunks, k=1):\n",
    "    _, I = index.search(q_emb.reshape(1, -1), k)\n",
    "    return [chunks[i] for i in I[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c1d601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(q_emb, index, chunks, k=1):\n",
    "    _, I = index.search(q_emb.reshape(1, -1), k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "def build_prompt(context, question):\n",
    "    return f\"\"\"\n",
    "Use ONLY the context to answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09e92247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_post(url, headers, json_data):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(total=3, backoff_factor=0.5)\n",
    "    session.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
    "    return session.post(url, headers=headers, json=json_data, timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4f4b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(context, question):\n",
    "    url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent\"\n",
    "    data = {\n",
    "        \"contents\": [{\"parts\": [{\"text\": build_prompt(context, question)}]}]\n",
    "    }\n",
    "    res = safe_post(f\"{url}?key={GEMINI_API_KEY}\", {\"Content-Type\": \"application/json\"}, data)\n",
    "    return res.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75780623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is React JS?\n",
      "A: React JS is an open-source JavaScript library developed by Facebook for building fast and interactive user interfaces, especially single-page applications.\n",
      "\n",
      "Q: What is supervised learning?\n",
      "A: Supervised learning is a type of machine learning where the model learns from labelled data. Each training example consists of input pairs and expected outputs (labels). The algorithms aim to map inputs to outputs.\n",
      "\n",
      "Q: What is Python?\n",
      "A: Python is a high-level, interpreted, object-oriented programming language known for its simplicity and readability.\n",
      "\n",
      "Q: What is Virtual DOM?\n",
      "A: Virtual DOM is a lightweight copy of the real DOM. React updates the Virtual DOM first and then efficiently updates only the changed parts in the real DOM.\n",
      "\n",
      "Q: Write a program to check palindrome?\n",
      "A: ```python\n",
      "s = input(\"Enter string: \")\n",
      "if s == s[::-1]: \n",
      "    print(\"Palindrome\")\n",
      "else: \n",
      "    print(\"Not Palindrome\")\n",
      "```\n",
      "\n",
      "Q: What is DBSCAN?\n",
      "A: DBSCAN is a density-based clustering algorithm that groups together points closely packed and marks as outliers points lying alone in low-density regions.\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# QUESTIONS & ANSWERS\n",
    "# ==========================\n",
    "faiss_index, chunks = load_faiss()\n",
    "\n",
    "questions = [\n",
    "    \"What is React JS?\",\n",
    "    \"What is supervised learning?\",\n",
    "    \"What is Python?\",\n",
    "    \"What is Virtual DOM?\",\n",
    "    \"Write a program to check palindrome?\",\n",
    "    \"What is DBSCAN?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    q_emb = embed_query(q)\n",
    "    top_chunk = retrieve(q_emb, faiss_index, chunks, k=1)\n",
    "    context = \"\\n\\n\".join(top_chunk)\n",
    "    ans = generate_answer(context, q)\n",
    "\n",
    "    print(\"\\nQ:\", q)\n",
    "    print(\"A:\", ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ce7ce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is React JS?\n",
      "A: React JS is an open-source JavaScript library developed by Facebook for building fast and interactive user interfaces, especially single-page applications.\n",
      "\n",
      "Q: What is supervised learning?\n",
      "A: Supervised learning is a type of machine learning where the model learns from labelled data. Each training example consists of input pairs and expected outputs (labels). The algorithms aim to map inputs to outputs.\n",
      "\n",
      "Q: What is Python?\n",
      "A: Python is a high-level, interpreted, object-oriented programming language known for its simplicity and readability.\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from docx import Document as DocxDocument\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.docstore.document import Document\n",
    "import faiss\n",
    "import numpy as np\n",
    "import tempfile, os, re, pickle\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ==========================\n",
    "# Azure Blob Config\n",
    "# ==========================\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "\n",
    "\n",
    "env_path = Path.cwd() / \".env\"\n",
    "load_dotenv(env_path)\n",
    "\n",
    "AZURE_CONNECTION_STRING = os.getenv(\"AZURE_CONNECTION_STRING\")\n",
    "CONTAINER_NAME = os.getenv(\"CONTAINER_NAME\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_GEMINI_API_KEY\")\n",
    "\n",
    "# ==========================\n",
    "# Text cleaning\n",
    "# ==========================\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# ==========================\n",
    "# Load documents (PDF + DOCX + TXT)\n",
    "# ==========================\n",
    "documents_text = []\n",
    "\n",
    "for blob in container_client.list_blobs():\n",
    "    if not blob.name.lower().endswith((\".pdf\", \".docx\", \".txt\")):\n",
    "        continue\n",
    "\n",
    "    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(blob.name)[1])\n",
    "    tmp_file.write(container_client.get_blob_client(blob.name).download_blob().readall())\n",
    "    tmp_file.close()\n",
    "\n",
    "    ext = os.path.splitext(blob.name)[1].lower()\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        text = \"\\n\".join([p.page_content for p in PyPDFLoader(tmp_file.name).load()])\n",
    "\n",
    "    elif ext == \".docx\":\n",
    "        doc = DocxDocument(tmp_file.name)\n",
    "        text = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "\n",
    "    elif ext == \".txt\":\n",
    "        with open(tmp_file.name, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "    os.remove(tmp_file.name)\n",
    "    documents_text.append(clean_text(text))\n",
    "\n",
    "# ==========================\n",
    "# Chunking\n",
    "# ==========================\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_chunks = []\n",
    "for txt in documents_text:\n",
    "    all_chunks.extend(splitter.split_text(txt))\n",
    "\n",
    "# ==========================\n",
    "# Embeddings + FAISS\n",
    "# ==========================\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    api_key=GEMINI_API_KEY\n",
    ")\n",
    "\n",
    "documents = [Document(page_content=c) for c in all_chunks]\n",
    "embeddings = embedding_model.embed_documents([d.page_content for d in documents])\n",
    "embeddings_np = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(embeddings_np.shape[1])\n",
    "faiss_index.add(embeddings_np)\n",
    "\n",
    "faiss.write_index(faiss_index, \"faiss_index.idx\")\n",
    "with open(\"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "# ==========================\n",
    "# Helpers\n",
    "# ==========================\n",
    "def load_faiss():\n",
    "    return faiss.read_index(\"faiss_index.idx\"), pickle.load(open(\"chunks.pkl\", \"rb\"))\n",
    "\n",
    "def embed_query(q):\n",
    "    return np.array(\n",
    "        embedding_model.embed_query(q),\n",
    "        dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "def retrieve(q_emb, index, chunks, k=1):\n",
    "    _, I = index.search(q_emb.reshape(1, -1), k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "def build_prompt(context, question):\n",
    "    return f\"\"\"\n",
    "Use ONLY the context to answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def safe_post(url, headers, json_data):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(total=3, backoff_factor=0.5)\n",
    "    session.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
    "    return session.post(url, headers=headers, json=json_data, timeout=30)\n",
    "\n",
    "def generate_answer(context, question):\n",
    "    url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent\"\n",
    "    data = {\n",
    "        \"contents\": [{\"parts\": [{\"text\": build_prompt(context, question)}]}]\n",
    "    }\n",
    "    res = safe_post(f\"{url}?key={GEMINI_API_KEY}\", {\"Content-Type\": \"application/json\"}, data)\n",
    "    return res.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "\n",
    "# ==========================\n",
    "# QUESTIONS & ANSWERS\n",
    "# ==========================\n",
    "faiss_index, chunks = load_faiss()\n",
    "\n",
    "questions = [\n",
    "    \"What is React JS?\",\n",
    "    \"What is supervised learning?\",\n",
    "    \"What is Python?\"\n",
    "]\n",
    "for q in questions:\n",
    "    q_emb = embed_query(q)\n",
    "    top_chunk = retrieve(q_emb, faiss_index, chunks, k=1)\n",
    "    context = \"\\n\\n\".join(top_chunk)\n",
    "    ans = generate_answer(context, q)\n",
    "    print(\"\\nQ:\", q)\n",
    "    print(\"A:\", ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55d47fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to container: kalyanpdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is React JS?\n",
      "A: React JS is an open-source JavaScript library developed by Facebook for building fast and interactive user interfaces, especially single-page applications.\n",
      "\n",
      "Q: What is supervised learning?\n",
      "A: Supervised learning is a type of machine learning where the model learns from labelled data. Each training example consists of input pairs and expected outputs (labels). The algorithms aim to map inputs to outputs. Example: Classification (e.g., spam detection), Regression (e.g., house price prediction).\n",
      "\n",
      "Q: What is Python?\n",
      "A: Python is a high-level, interpreted, object-oriented programming language known for its simplicity and readability.\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# IMPORTS\n",
    "# ==========================\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from docx import Document as DocxDocument\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.docstore.document import Document\n",
    "\n",
    "# ==========================\n",
    "# LOAD ENV\n",
    "# ==========================\n",
    "# Force load .env in the notebook folder\n",
    "env_path = Path.cwd() / \".env\"\n",
    "load_dotenv(env_path)\n",
    "\n",
    "AZURE_CONNECTION_STRING = os.getenv(\"AZURE_CONNECTION_STRING\")\n",
    "CONTAINER_NAME = os.getenv(\"CONTAINER_NAME\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_GEMINI_API_KEY\")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# CONNECT TO AZURE BLOB\n",
    "# ==========================\n",
    "blob_service_client = BlobServiceClient.from_connection_string(AZURE_CONNECTION_STRING)\n",
    "container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
    "print(\"Connected to container:\", CONTAINER_NAME)\n",
    "\n",
    "# ==========================\n",
    "# TEXT CLEANING FUNCTION\n",
    "# ==========================\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# ==========================\n",
    "# LOAD DOCUMENTS\n",
    "# ==========================\n",
    "documents_text = []\n",
    "\n",
    "for blob in container_client.list_blobs():\n",
    "    if not blob.name.lower().endswith((\".pdf\", \".docx\", \".txt\")):\n",
    "        continue\n",
    "\n",
    "    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(blob.name)[1])\n",
    "    tmp_file.write(container_client.get_blob_client(blob.name).download_blob().readall())\n",
    "    tmp_file.close()\n",
    "\n",
    "    ext = os.path.splitext(blob.name)[1].lower()\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        text = \"\\n\".join([p.page_content for p in PyPDFLoader(tmp_file.name).load()])\n",
    "    elif ext == \".docx\":\n",
    "        doc = DocxDocument(tmp_file.name)\n",
    "        text = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "    elif ext == \".txt\":\n",
    "        with open(tmp_file.name, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "    os.remove(tmp_file.name)\n",
    "    documents_text.append(clean_text(text))\n",
    "\n",
    "# ==========================\n",
    "# CHUNKING\n",
    "# ==========================\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_chunks = []\n",
    "for txt in documents_text:\n",
    "    all_chunks.extend(splitter.split_text(txt))\n",
    "\n",
    "# ==========================\n",
    "# EMBEDDINGS + FAISS\n",
    "# ==========================\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    api_key=GEMINI_API_KEY\n",
    ")\n",
    "\n",
    "documents = [Document(page_content=c) for c in all_chunks]\n",
    "embeddings = embedding_model.embed_documents([d.page_content for d in documents])\n",
    "embeddings_np = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(embeddings_np.shape[1])\n",
    "faiss_index.add(embeddings_np)\n",
    "\n",
    "faiss.write_index(faiss_index, \"faiss_index.idx\")\n",
    "with open(\"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "\n",
    "# ==========================\n",
    "# HELPER FUNCTIONS\n",
    "# ==========================\n",
    "def load_faiss():\n",
    "    return faiss.read_index(\"faiss_index.idx\"), pickle.load(open(\"chunks.pkl\", \"rb\"))\n",
    "\n",
    "def embed_query(q):\n",
    "    return np.array(embedding_model.embed_query(q), dtype=\"float32\")\n",
    "\n",
    "def retrieve(q_emb, index, chunks, k=1):\n",
    "    _, I = index.search(q_emb.reshape(1, -1), k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "def build_prompt(context, question):\n",
    "    return f\"Use ONLY the context to answer.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "def safe_post(url, headers, json_data):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(total=3, backoff_factor=0.5)\n",
    "    session.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
    "    return session.post(url, headers=headers, json=json_data, timeout=30)\n",
    "\n",
    "def generate_answer(context, question):\n",
    "    url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent\"\n",
    "    data = {\"contents\": [{\"parts\": [{\"text\": build_prompt(context, question)}]}]}\n",
    "    res = safe_post(f\"{url}?key={GEMINI_API_KEY}\", {\"Content-Type\": \"application/json\"}, data)\n",
    "    return res.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "\n",
    "# ==========================\n",
    "# EXAMPLE QUESTIONS\n",
    "# ==========================\n",
    "faiss_index, chunks = load_faiss()\n",
    "\n",
    "questions = [\n",
    "    \"What is React JS?\",\n",
    "    \"What is supervised learning?\",\n",
    "    \"What is Python?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    q_emb = embed_query(q)\n",
    "    top_chunk = retrieve(q_emb, faiss_index, chunks, k=1)\n",
    "    context = \"\\n\\n\".join(top_chunk)\n",
    "    ans = generate_answer(context, q)\n",
    "    print(\"\\nQ:\", q)\n",
    "    print(\"A:\", ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2974af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY: AIzaSyAbVjU3k7xMPAw09qr4tQZ84JZAfoRp2XM\n",
      "AZURE_CONNECTION_STRING: True\n",
      "CONTAINER_NAME: kalyanpdf\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "load_dotenv(Path.cwd() / \".env\")\n",
    "\n",
    "print(\"GEMINI_API_KEY:\", os.getenv(\"GEMINI_API_KEY\"))\n",
    "print(\"AZURE_CONNECTION_STRING:\", os.getenv(\"AZURE_CONNECTION_STRING\") is not None)\n",
    "print(\"CONTAINER_NAME:\", os.getenv(\"CONTAINER_NAME\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc6662d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
